{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Two-Head网络对OCSVM进行改进"
   ]
  },
  {
   "source": [
    "## 数据预处理"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import visdom\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# tensorflowGPU的设置\n",
    "\n",
    "# tf.config.experimental.list_physical_devices('CPU')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' #屏蔽通知信息，输出warning, Error, fatal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              length\ncount   12402.000000\nmean     5652.612804\nstd     20251.140030\nmin         2.000000\n25%       202.000000\n50%      1105.500000\n75%      2849.000000\nmax    780386.000000\n未采样前数据集的规模： (6104, 5) (2642, 5) (3656, 5)\n采样后数据集的规模： (6104, 5) (1849, 5) (1828, 5)\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "def judge_1(time):\n",
    "    time = time[:7].replace('-', '')\n",
    "    if time <= '201803':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def judge_2(time):\n",
    "    time = time[:7].replace('-', '')\n",
    "    if time > '201803':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "name2label = {'trojan':0, 'virus':1, 'worm':2, 'backdoor':3}\n",
    "data_csv = pd.read_csv('res_handle.csv')\n",
    "print(data_csv.describe())\n",
    "data_2017 = data_csv[data_csv['first_seen'].apply(lambda x: x[:4]) == '2017']\n",
    "data_2018 = data_csv[data_csv['first_seen'].apply(lambda x: x[:4]) == '2018']\n",
    "data_2019 = data_csv[data_csv['first_seen'].apply(lambda x: x[:4]) == '2019']\n",
    "# 将201803之前的数据划分到2017数据集下    \n",
    "data_2018_1 = data_2018[data_2018['first_seen'].apply(judge_1)]\n",
    "data_2018_2 = data_2018[data_2018['first_seen'].apply(judge_2)]\n",
    "\n",
    "data_train = data_2017.append(data_2018_１)\n",
    "data_test_1 = data_2018_２\n",
    "data_test_2 = data_2019\n",
    "print(\"未采样前数据集的规模：\", data_train.shape, data_test_1.shape, data_test_2.shape)\n",
    "\n",
    "# 对测试集进行下采样\n",
    "data_train = data_train.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "data_test_1 = data_test_1.sample(frac=0.7, random_state=1).reset_index(drop=True)\n",
    "data_test_2 = data_test_2.sample(frac=0.5, random_state=1).reset_index(drop=True)\n",
    "print(\"采样后数据集的规模：\", data_train.shape, data_test_1.shape, data_test_2.shape)\n",
    "\n",
    "#获取数据的路径及标签\n",
    "data_train_names, labels_train = name_loader(data_train)\n",
    "data_test_1_names, labels_test_1 = name_loader(data_test_1)\n",
    "data_test_2_names, labels_test_2 = name_loader(data_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 12402/12402 [00:07<00:00, 1663.09it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "641"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# 利用labelencoder构建opcode编码器\n",
    "opcode_all = []\n",
    "for idx in tqdm(range(data_csv.shape[0])):\n",
    "    opcode_str = data_csv.at[idx, 'opcode'].split()\n",
    "    opcode_all += opcode_str\n",
    "    opcode_all = list(set(opcode_all))\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(opcode_all)\n",
    "len(list(le.classes_))"
   ]
  },
  {
   "source": [
    "## 训练two-head模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建TextCNN类\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\n",
    "\n",
    "class TextCNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=4,\n",
    "                 last_activation=None):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        # Embedding part can try multichannel as same as origin paper\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        convs = []\n",
    "        for kernel_size in [2, 3, 4, 5]:\n",
    "            c = Conv1D(128, kernel_size, activation='relu')(embedding)\n",
    "            c = GlobalMaxPooling1D()(c)\n",
    "            convs.append(c)\n",
    "        x = Concatenate()(convs)\n",
    "        d_1 = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01))(x)\n",
    "        d_2 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01))(d_1)\n",
    "\n",
    "        output_1 = Dense(self.class_num, activation=self.last_activation)(d_2)\n",
    "        output_2 = Dense(self.class_num, activation=self.last_activation)(d_2)\n",
    "        model = Model(inputs=input, outputs=[output_1, output_2])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(res, max_len=2850):\n",
    "    length = len(res)\n",
    "    if length > max_len:\n",
    "        return res[ :max_len]\n",
    "    elif length < max_len:\n",
    "        return res + [0]*(max_len-length)\n",
    "    return res\n",
    "\n",
    "\n",
    "def train_data_generator(codes, labels, mode):\n",
    "    \n",
    "    if mode == 'train':    \n",
    "        codes = codes[: 5100]\n",
    "        labels = labels[: 5100]\n",
    "    elif mode == 'val':    \n",
    "        codes = codes[5100: ]\n",
    "        labels = labels[5100: ]\n",
    "\n",
    "    labels_res = np.eye(4)[labels]\n",
    "    for idx in range(len(codes)):\n",
    "        fn = codes[idx]\n",
    "        fn = bytes.decode(fn)\n",
    "        op_string = data_csv[data_csv.name == fn]\n",
    "        if len(op_string) == 0:\n",
    "            print('null')\n",
    "            continue\n",
    "        op_string = op_string.iloc[0, 1].split()\n",
    "        res = list(le.transform(op_string))\n",
    "        res = pad_data(res)\n",
    "        yield res, labels_res[idx]\n",
    "        \n",
    "        \n",
    "def train_data_loader(codes, labels, mode):\n",
    "    \n",
    "    if mode == 'train':    \n",
    "        codes = codes[: 5100]\n",
    "        labels = labels[: 5100]\n",
    "    elif mode == 'val':    \n",
    "        codes = codes[5100: ]\n",
    "        labels = labels[5100: ]\n",
    "    \n",
    "    labels_res = np.eye(4)[labels]\n",
    "    res_all = []\n",
    "    for idx in tqdm(range(len(codes))):\n",
    "        fn = codes[idx]\n",
    "        op_string = data_csv[data_csv.name == fn]\n",
    "        op_string = op_string.iloc[0, 1].split()\n",
    "        res = list(le.transform(op_string))\n",
    "        res = pad_data(res)\n",
    "        res_all.append(res)\n",
    "    return np.array(res_all), labels_res\n",
    "\n",
    "\n",
    "def codes_loader(codes):\n",
    "    \n",
    "    res_all = []\n",
    "    for idx in tqdm(range(len(codes))):\n",
    "        fn = codes[idx]\n",
    "        op_string = data_csv[data_csv.name == fn]\n",
    "        op_string = op_string.iloc[0, 1].split()\n",
    "        res = list(le.transform(op_string))\n",
    "        res = pad_data(res)\n",
    "        res_all.append(res)\n",
    "    return np.array(res_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 2.7653419971466064\n",
      "Training loss (for one batch) at step 10: 2.726194381713867\n",
      "Training loss (for one batch) at step 20: 2.5980563163757324\n",
      "Training loss (for one batch) at step 30: 2.2676656246185303\n",
      "Training loss (for one batch) at step 40: 2.0087127685546875\n",
      "Training acc over epoch: 0.4954128563404083 0.4906618595123291\n",
      "Validation acc: 0.7187090516090393 0.6587483882904053\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 1.6196199655532837\n",
      "Training loss (for one batch) at step 10: 1.7780039310455322\n",
      "Training loss (for one batch) at step 20: 1.352810263633728\n",
      "Training loss (for one batch) at step 30: 0.895261824131012\n",
      "Training loss (for one batch) at step 40: 1.026787281036377\n",
      "Training acc over epoch: 0.7961992025375366 0.8006225228309631\n",
      "Validation acc: 0.8648427128791809 0.8682830929756165\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.7006429433822632\n",
      "Training loss (for one batch) at step 10: 1.0168956518173218\n",
      "Training loss (for one batch) at step 20: 0.6920450925827026\n",
      "Training loss (for one batch) at step 30: 0.4108438491821289\n",
      "Training loss (for one batch) at step 40: 0.6073373556137085\n",
      "Training acc over epoch: 0.9056356549263 0.9100589752197266\n",
      "Validation acc: 0.9346330165863037 0.9318479895591736\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.34894272685050964\n",
      "Training loss (for one batch) at step 10: 0.6255942583084106\n",
      "Training loss (for one batch) at step 20: 0.39371854066848755\n",
      "Training loss (for one batch) at step 30: 0.2733950614929199\n",
      "Training loss (for one batch) at step 40: 0.42952418327331543\n",
      "Training acc over epoch: 0.9356160163879395 0.9359436631202698\n",
      "Validation acc: 0.9467562437057495 0.9464285969734192\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.2340795248746872\n",
      "Training loss (for one batch) at step 10: 0.426388144493103\n",
      "Training loss (for one batch) at step 20: 0.3004876375198364\n",
      "Training loss (for one batch) at step 30: 0.2037881761789322\n",
      "Training loss (for one batch) at step 40: 0.36046573519706726\n",
      "Training acc over epoch: 0.9485583305358887 0.9470838904380798\n",
      "Validation acc: 0.9587156176567078 0.9592070579528809\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.18655641376972198\n",
      "Training loss (for one batch) at step 10: 0.35959893465042114\n",
      "Training loss (for one batch) at step 20: 0.2740882933139801\n",
      "Training loss (for one batch) at step 30: 0.1763187050819397\n",
      "Training loss (for one batch) at step 40: 0.3344632387161255\n",
      "Training acc over epoch: 0.9547837376594543 0.9541284441947937\n",
      "Validation acc: 0.9662516117095947 0.9655963182449341\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.16723695397377014\n",
      "Training loss (for one batch) at step 10: 0.30238622426986694\n",
      "Training loss (for one batch) at step 20: 0.2743028402328491\n",
      "Training loss (for one batch) at step 30: 0.17389258742332458\n",
      "Training loss (for one batch) at step 40: 0.22462773323059082\n",
      "Training acc over epoch: 0.960026204586029 0.9590432643890381\n",
      "Validation acc: 0.9734600186347961 0.9736238718032837\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 0.16036677360534668\n",
      "Training loss (for one batch) at step 10: 0.23828324675559998\n",
      "Training loss (for one batch) at step 20: 0.21813395619392395\n",
      "Training loss (for one batch) at step 30: 0.15067435801029205\n",
      "Training loss (for one batch) at step 40: 0.14179103076457977\n",
      "Training acc over epoch: 0.9670707583427429 0.966087818145752\n",
      "Validation acc: 0.976245105266571 0.9760812520980835\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 0.16505087912082672\n",
      "Training loss (for one batch) at step 10: 0.18523955345153809\n",
      "Training loss (for one batch) at step 20: 0.17915087938308716\n",
      "Training loss (for one batch) at step 30: 0.11657264828681946\n",
      "Training loss (for one batch) at step 40: 0.11732083559036255\n",
      "Training acc over epoch: 0.9736238718032837 0.9732962250709534\n",
      "Validation acc: 0.9801769256591797 0.9793577790260315\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 0.13232913613319397\n",
      "Training loss (for one batch) at step 10: 0.16827726364135742\n",
      "Training loss (for one batch) at step 20: 0.16086754202842712\n",
      "Training loss (for one batch) at step 30: 0.10717082023620605\n",
      "Training loss (for one batch) at step 40: 0.09325699508190155\n",
      "Training acc over epoch: 0.9760812520980835 0.9765727519989014\n",
      "Validation acc: 0.9813237190246582 0.9816513657569885\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 0.10775198042392731\n",
      "Training loss (for one batch) at step 10: 0.14958979189395905\n",
      "Training loss (for one batch) at step 20: 0.14188089966773987\n",
      "Training loss (for one batch) at step 30: 0.10169964283704758\n",
      "Training loss (for one batch) at step 40: 0.07966053485870361\n",
      "Training acc over epoch: 0.9793577790260315 0.9791939854621887\n",
      "Validation acc: 0.9834534525871277 0.9832896590232849\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 0.094794362783432\n",
      "Training loss (for one batch) at step 10: 0.13902126252651215\n",
      "Training loss (for one batch) at step 20: 0.12629671394824982\n",
      "Training loss (for one batch) at step 30: 0.10222641378641129\n",
      "Training loss (for one batch) at step 40: 0.06892065703868866\n",
      "Training acc over epoch: 0.9809960722923279 0.9813237190246582\n",
      "Validation acc: 0.9855832457542419 0.9855832457542419\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 0.09305468201637268\n",
      "Training loss (for one batch) at step 10: 0.13751724362373352\n",
      "Training loss (for one batch) at step 20: 0.11076047271490097\n",
      "Training loss (for one batch) at step 30: 0.11029534041881561\n",
      "Training loss (for one batch) at step 40: 0.05956786125898361\n",
      "Training acc over epoch: 0.9831258058547974 0.9826343655586243\n",
      "Validation acc: 0.9859108924865723 0.9859108924865723\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 0.0981738418340683\n",
      "Training loss (for one batch) at step 10: 0.13867256045341492\n",
      "Training loss (for one batch) at step 20: 0.10337924212217331\n",
      "Training loss (for one batch) at step 30: 0.11321301013231277\n",
      "Training loss (for one batch) at step 40: 0.05530295521020889\n",
      "Training acc over epoch: 0.9841088056564331 0.9846002459526062\n",
      "Validation acc: 0.9862385392189026 0.9855832457542419\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 0.0999484658241272\n",
      "Training loss (for one batch) at step 10: 0.13561689853668213\n",
      "Training loss (for one batch) at step 20: 0.10365080833435059\n",
      "Training loss (for one batch) at step 30: 0.1162840873003006\n",
      "Training loss (for one batch) at step 40: 0.05057525634765625\n",
      "Training acc over epoch: 0.9842725992202759 0.9847640991210938\n",
      "Validation acc: 0.9868938326835632 0.9862385392189026\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 0.09548044204711914\n",
      "Training loss (for one batch) at step 10: 0.12596991658210754\n",
      "Training loss (for one batch) at step 20: 0.10226142406463623\n",
      "Training loss (for one batch) at step 30: 0.1202348843216896\n",
      "Training loss (for one batch) at step 40: 0.04626952484250069\n",
      "Training acc over epoch: 0.9849278926849365 0.9852555990219116\n",
      "Validation acc: 0.9873853325843811 0.9870576858520508\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 0.08985918760299683\n",
      "Training loss (for one batch) at step 10: 0.11565036326646805\n",
      "Training loss (for one batch) at step 20: 0.09694957733154297\n",
      "Training loss (for one batch) at step 30: 0.1252075433731079\n",
      "Training loss (for one batch) at step 40: 0.044310830533504486\n",
      "Training acc over epoch: 0.9852555990219116 0.9857470393180847\n",
      "Validation acc: 0.9873853325843811 0.9870576858520508\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 0.08915784955024719\n",
      "Training loss (for one batch) at step 10: 0.10788542032241821\n",
      "Training loss (for one batch) at step 20: 0.09300228953361511\n",
      "Training loss (for one batch) at step 30: 0.1236078217625618\n",
      "Training loss (for one batch) at step 40: 0.04726921766996384\n",
      "Training acc over epoch: 0.9859108924865723 0.9862385392189026\n",
      "Validation acc: 0.9882044792175293 0.9872214794158936\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 0.0926116406917572\n",
      "Training loss (for one batch) at step 10: 0.10435566306114197\n",
      "Training loss (for one batch) at step 20: 0.08852913975715637\n",
      "Training loss (for one batch) at step 30: 0.11448074877262115\n",
      "Training loss (for one batch) at step 40: 0.05356856435537338\n",
      "Training acc over epoch: 0.9867300391197205 0.9865661859512329\n",
      "Validation acc: 0.9872214794158936 0.9870576858520508\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 0.09942914545536041\n",
      "Training loss (for one batch) at step 10: 0.10536554455757141\n",
      "Training loss (for one batch) at step 20: 0.08492815494537354\n",
      "Training loss (for one batch) at step 30: 0.09101753681898117\n",
      "Training loss (for one batch) at step 40: 0.06742839515209198\n",
      "Training acc over epoch: 0.9857470393180847 0.9854193925857544\n",
      "Validation acc: 0.9870576858520508 0.9870576858520508\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 0.1036268100142479\n",
      "Training loss (for one batch) at step 10: 0.11383800953626633\n",
      "Training loss (for one batch) at step 20: 0.0824209600687027\n",
      "Training loss (for one batch) at step 30: 0.07472643256187439\n",
      "Training loss (for one batch) at step 40: 0.09679962694644928\n",
      "Training acc over epoch: 0.983781099319458 0.9839449524879456\n",
      "Validation acc: 0.9867300391197205 0.9867300391197205\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 0.10394639521837234\n",
      "Training loss (for one batch) at step 10: 0.15137389302253723\n",
      "Training loss (for one batch) at step 20: 0.07639618963003159\n",
      "Training loss (for one batch) at step 30: 0.05929458886384964\n",
      "Training loss (for one batch) at step 40: 0.18788988888263702\n",
      "Training acc over epoch: 0.9819790124893188 0.9819790124893188\n",
      "Validation acc: 0.9870576858520508 0.9862385392189026\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 0.1013842225074768\n",
      "Training loss (for one batch) at step 10: 0.22424231469631195\n",
      "Training loss (for one batch) at step 20: 0.07491305470466614\n",
      "Training loss (for one batch) at step 30: 0.048636727035045624\n",
      "Training loss (for one batch) at step 40: 0.1338575780391693\n",
      "Training acc over epoch: 0.9801769256591797 0.9796854257583618\n",
      "Validation acc: 0.9800131320953369 0.978210985660553\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 0.119411900639534\n",
      "Training loss (for one batch) at step 10: 0.22903716564178467\n",
      "Training loss (for one batch) at step 20: 0.10995759814977646\n",
      "Training loss (for one batch) at step 30: 0.05380336940288544\n",
      "Training loss (for one batch) at step 40: 0.03693389520049095\n",
      "Training acc over epoch: 0.9800131320953369 0.9803407788276672\n",
      "Validation acc: 0.9873853325843811 0.9873853325843811\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 0.07855602353811264\n",
      "Training loss (for one batch) at step 10: 0.09885527193546295\n",
      "Training loss (for one batch) at step 20: 0.0857163518667221\n",
      "Training loss (for one batch) at step 30: 0.12559929490089417\n",
      "Training loss (for one batch) at step 40: 0.04321885108947754\n",
      "Training acc over epoch: 0.9832896590232849 0.9829620122909546\n",
      "Validation acc: 0.9877129793167114 0.9877129793167114\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 0.0562678799033165\n",
      "Training loss (for one batch) at step 10: 0.08415210247039795\n",
      "Training loss (for one batch) at step 20: 0.1127871423959732\n",
      "Training loss (for one batch) at step 30: 0.07288975268602371\n",
      "Training loss (for one batch) at step 40: 0.07758182287216187\n",
      "Training acc over epoch: 0.9826343655586243 0.9824705123901367\n",
      "Validation acc: 0.9829620122909546 0.982798159122467\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 0.1176416277885437\n",
      "Training loss (for one batch) at step 10: 0.11559738218784332\n",
      "Training loss (for one batch) at step 20: 0.15168938040733337\n",
      "Training loss (for one batch) at step 30: 0.09300895780324936\n",
      "Training loss (for one batch) at step 40: 0.18711724877357483\n",
      "Training acc over epoch: 0.9783748388290405 0.9788663387298584\n",
      "Validation acc: 0.9615006446838379 0.9629750847816467\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 0.21621155738830566\n",
      "Training loss (for one batch) at step 10: 0.08367303013801575\n",
      "Training loss (for one batch) at step 20: 0.16909880936145782\n",
      "Training loss (for one batch) at step 30: 0.11121518909931183\n",
      "Training loss (for one batch) at step 40: 0.06786268204450607\n",
      "Training acc over epoch: 0.9760812520980835 0.9764088988304138\n",
      "Validation acc: 0.9765727519989014 0.9769003987312317\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 0.15871095657348633\n",
      "Training loss (for one batch) at step 10: 0.2519770562648773\n",
      "Training loss (for one batch) at step 20: 0.3493688404560089\n",
      "Training loss (for one batch) at step 30: 0.1024295911192894\n",
      "Training loss (for one batch) at step 40: 0.1522214710712433\n",
      "Training acc over epoch: 0.9718217849731445 0.9711664319038391\n",
      "Validation acc: 0.9882044792175293 0.9880406260490417\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 0.10047818720340729\n",
      "Training loss (for one batch) at step 10: 0.1411377489566803\n",
      "Training loss (for one batch) at step 20: 0.1813448965549469\n",
      "Training loss (for one batch) at step 30: 0.04441849887371063\n",
      "Training loss (for one batch) at step 40: 0.04064099118113518\n",
      "Training acc over epoch: 0.9819790124893188 0.9823066592216492\n",
      "Validation acc: 0.9855832457542419 0.9852555990219116\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 0.05791059881448746\n",
      "Training loss (for one batch) at step 10: 0.11888126283884048\n",
      "Training loss (for one batch) at step 20: 0.12184616923332214\n",
      "Training loss (for one batch) at step 30: 0.05654748156666756\n",
      "Training loss (for one batch) at step 40: 0.04084304720163345\n",
      "Training acc over epoch: 0.9855832457542419 0.9859108924865723\n",
      "Validation acc: 0.9854193925857544 0.9850917458534241\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 0.06004594266414642\n",
      "Training loss (for one batch) at step 10: 0.12589645385742188\n",
      "Training loss (for one batch) at step 20: 0.10207810252904892\n",
      "Training loss (for one batch) at step 30: 0.052410759031772614\n",
      "Training loss (for one batch) at step 40: 0.05357681214809418\n",
      "Training acc over epoch: 0.9865661859512329 0.9875491261482239\n",
      "Validation acc: 0.9859108924865723 0.9852555990219116\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 0.06464415788650513\n",
      "Training loss (for one batch) at step 10: 0.1327892541885376\n",
      "Training loss (for one batch) at step 20: 0.09390879422426224\n",
      "Training loss (for one batch) at step 30: 0.04457258805632591\n",
      "Training loss (for one batch) at step 40: 0.056891486048698425\n",
      "Training acc over epoch: 0.9864023327827454 0.9867300391197205\n",
      "Validation acc: 0.9882044792175293 0.9880406260490417\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 0.061154067516326904\n",
      "Training loss (for one batch) at step 10: 0.13228823244571686\n",
      "Training loss (for one batch) at step 20: 0.07760737091302872\n",
      "Training loss (for one batch) at step 30: 0.04568459093570709\n",
      "Training loss (for one batch) at step 40: 0.07907943427562714\n",
      "Training acc over epoch: 0.9867300391197205 0.9870576858520508\n",
      "Validation acc: 0.9882044792175293 0.9878767728805542\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 0.05281369388103485\n",
      "Training loss (for one batch) at step 10: 0.10391828417778015\n",
      "Training loss (for one batch) at step 20: 0.06096480414271355\n",
      "Training loss (for one batch) at step 30: 0.049745507538318634\n",
      "Training loss (for one batch) at step 40: 0.06657430529594421\n",
      "Training acc over epoch: 0.9875491261482239 0.9875491261482239\n",
      "Validation acc: 0.9890235662460327 0.9886959195137024\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 0.04670334234833717\n",
      "Training loss (for one batch) at step 10: 0.0924178883433342\n",
      "Training loss (for one batch) at step 20: 0.0594160333275795\n",
      "Training loss (for one batch) at step 30: 0.04707625135779381\n",
      "Training loss (for one batch) at step 40: 0.07880832254886627\n",
      "Training acc over epoch: 0.9872214794158936 0.9868938326835632\n",
      "Validation acc: 0.9865661859512329 0.9864023327827454\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 0.04318590834736824\n",
      "Training loss (for one batch) at step 10: 0.10170179605484009\n",
      "Training loss (for one batch) at step 20: 0.06783818453550339\n",
      "Training loss (for one batch) at step 30: 0.04620981961488724\n",
      "Training loss (for one batch) at step 40: 0.07108478248119354\n",
      "Training acc over epoch: 0.9868938326835632 0.9870576858520508\n",
      "Validation acc: 0.9859108924865723 0.9857470393180847\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 0.043060045689344406\n",
      "Training loss (for one batch) at step 10: 0.11160389333963394\n",
      "Training loss (for one batch) at step 20: 0.0630437582731247\n",
      "Training loss (for one batch) at step 30: 0.04579358547925949\n",
      "Training loss (for one batch) at step 40: 0.07614415138959885\n",
      "Training acc over epoch: 0.9865661859512329 0.9872214794158936\n",
      "Validation acc: 0.9859108924865723 0.9857470393180847\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 0.043401509523391724\n",
      "Training loss (for one batch) at step 10: 0.10592252016067505\n",
      "Training loss (for one batch) at step 20: 0.05835859850049019\n",
      "Training loss (for one batch) at step 30: 0.04377325251698494\n",
      "Training loss (for one batch) at step 40: 0.0649908185005188\n",
      "Training acc over epoch: 0.9867300391197205 0.9868938326835632\n",
      "Validation acc: 0.9864023327827454 0.9864023327827454\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 0.04279989004135132\n",
      "Training loss (for one batch) at step 10: 0.09826409816741943\n",
      "Training loss (for one batch) at step 20: 0.05657162517309189\n",
      "Training loss (for one batch) at step 30: 0.04276563227176666\n",
      "Training loss (for one batch) at step 40: 0.071454718708992\n",
      "Training acc over epoch: 0.9868938326835632 0.9868938326835632\n",
      "Validation acc: 0.9865661859512329 0.9867300391197205\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input, Model, regularizers\n",
    "from tensorflow import keras\n",
    "\n",
    "# 实例化TextCNN模型\n",
    "max_len = 2850\n",
    "max_features = 641\n",
    "embedding_dims = 16\n",
    "model = TextCNN(max_len, max_features, embedding_dims).get_model()\n",
    "\n",
    "# 创建dataset对象\n",
    "batch_size = 128\n",
    "max_len = 2850\n",
    "db_train = tf.data.Dataset.from_generator(train_data_generator, args=[data_train_names, labels_train, 'train'], \n",
    "                                          output_types=(tf.float32, tf.int32), output_shapes=((None,), (4, )))\n",
    "db_val = tf.data.Dataset.from_generator(train_data_generator, args=[data_train_names, labels_train, 'val'], \n",
    "                                          output_types=(tf.float32, tf.int32), output_shapes=((None,), (4, )))\n",
    "\n",
    "# 创建训练过程\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_sup = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "metrics_train_1 = keras.metrics.CategoricalAccuracy()\n",
    "metrics_train_2 = keras.metrics.CategoricalAccuracy()\n",
    "metrics_val_1 = keras.metrics.CategoricalAccuracy()\n",
    "metrics_val_2 = keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "epochs = 40\n",
    "db_train = db_train.batch(batch_size)\n",
    "db_val = db_val.batch(batch_size)\n",
    "\n",
    "loss_all = []\n",
    "acc_all = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(db_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            out1, out2 = model(x_batch_train)\n",
    "            loss_value = loss_sup(y_batch_train, out1) + loss_sup(y_batch_train, out2)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        metrics_train_1(y_batch_train, out1)\n",
    "        metrics_train_2(y_batch_train, out2)\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            loss_all.append(loss_value)\n",
    "            print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "    train_acc_1 = metrics_train_1.result()\n",
    "    train_acc_2 = metrics_train_2.result()\n",
    "    print('Training acc over epoch: %s %s' % (float(train_acc_1), float(train_acc_2)))\n",
    "    metrics_train_1.reset_states()\n",
    "    metrics_train_2.reset_states()\n",
    "    \n",
    "    for x_batch_val, y_batch_val in db_val:\n",
    "        val_out_1, val_out_2 = model(x_batch_val)\n",
    "        metrics_val_1(y_batch_val, val_out_1)\n",
    "        metrics_val_2(y_batch_val, val_out_2)\n",
    "    \n",
    "    val_acc_1 = metrics_val_1.result()\n",
    "    val_acc_2 = metrics_val_2.result()\n",
    "    acc_all.append(val_acc_1)\n",
    "    print('Validation acc: %s %s' % (float(val_acc_1), float(val_acc_2))) \n",
    "    metrics_val_1.reset_states()\n",
    "    metrics_val_2.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model.save('./model/textcnn_twohead.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制损失函数曲线\n",
    "p1 = plt.figure(figsize=(5, 3.5),dpi=300)\n",
    "plt.plot(loss_all)\n",
    "plt.title(\"textcnn loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\"],loc=\"upper right\")\n",
    "plt.savefig(\"./figures/\" + \"twohead_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1849/1849 [00:13<00:00, 137.63it/s]\n",
      "Test AUROC: 0.923\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input, Model, regularizers\n",
    "from tensorflow import keras\n",
    "from utils import *\n",
    "\n",
    "max_len = 2850\n",
    "max_features = 641\n",
    "embedding_dims = 16\n",
    "model = TextCNN(max_len, max_features, embedding_dims).get_model()\n",
    "model.load_weights('./model/textcnn_twohead.h5')\n",
    "\n",
    "data_test_1 = pd.read_csv('data_test_1.csv')\n",
    "ood2label = {True: 0, False: 1}\n",
    "labels_test_1 = data_test_1['id'].map(lambda x: ood2label[x])\n",
    "labels_test_1 = labels_test_1.to_list()\n",
    "names_test_1 = data_test_1['name'].to_list()\n",
    "\n",
    "codes_test_1 = codes_loader(data_test_1_names)\n",
    "labels_test_1 = np.array(labels_test_1)\n",
    "test_1_auc, discs_1 = get_auc(model, codes_test_1, labels_test_1)\n",
    "print('Test AUROC: {:.3f}'.format(test_1_auc))\n",
    "plot_entropy(discs_1, labels_test_1, 'entropy_distribution_test1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_2 = pd.read_csv('data_test_2.csv')\n",
    "ood2label = {True: 0, False: 1}\n",
    "labels_test_2 = data_test_2['id'].map(lambda x: ood2label[x])\n",
    "labels_test_2 = labels_test_2.to_list()\n",
    "paths_test_2 = data_test_2['name'].to_list()\n",
    "codes_test_2 = codes_loader(data_test_2_names)\n",
    "labels_test_2 = np.array(labels_test_2)\n",
    "test_1_auc, discs_2 = get_auc(model, codes_test_2, labels_test_2)\n",
    "print('Test AUROC: {:.3f}'.format(test_1_auc))\n",
    "plot_entropy(discs_2, labels_test_２, 'entropy_distribution_test2.png')"
   ]
  },
  {
   "source": [
    "## 保存利用two-head异常检测模型得到的数据方便进行重训练"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.16262135922330098\nscore:  0.0014142394\n1849 1849\n                                                name  \\\n0  3fd80e5c6326b1debfb100402521cf1b7e6dd7cb5a8bcf...   \n1  51f19f2d2720a6533c0ce9ee72d0235f28e87531937f15...   \n2  4bcb1ba52a53b54dab1eef63abb56c19ce989e782d5060...   \n3  d7af87c0c1535f87196af6a00aa620073e36f25201f4a1...   \n4  b0ecefe0e9c334635c6f94b9d26963f41a668c8fcd748b...   \n\n                                              opcode  length     label  \\\n0  push mov add push mov retn mov push mov cmp jz...     203    trojan   \n1  call ldc.i4 nop ldc.i4 stloc.1 nop stloc.2 nop...   68302  backdoor   \n2  push add mov push call test jz movzx mov add p...    6740      worm   \n3                              mov cmp mov push retn       5      worm   \n4  nop call nop ret ldloca.s call stfld ldloca.s ...     199    trojan   \n\n   first_seen  novelty     id  \n0  2018-06-19        1   True  \n1  2018-06-18        1  False  \n2  2018-06-19        0  False  \n3  2018-06-19        1   True  \n4  2018-06-18        1   True  \n                                                name  \\\n0  9e2d0fb5305c55596f107457a9c77f993987cf83619f74...   \n1  8a8264794c465f15ca3c512663113da8a20eb1679c35b8...   \n2  67f78c02ba24aba5ca4150273473649b251db6fe0b3f78...   \n3  3fd80e5c6326b1debfb100402521cf1b7e6dd7cb5a8bcf...   \n4  461259b16d3cb2f68119c7c371ad9fbcd86d7ecbabdfdc...   \n\n                                              opcode  length   label  \\\n0  mov push mov push call retn cmp jz mov push mo...    1954   virus   \n1  ldarg.0 call ret ldarg.0 call ret newobj stsfl...     181  trojan   \n2  pusha call pop add mov add sub mov lodsw shl m...     223  trojan   \n3  push mov add push mov retn mov push mov cmp jz...     203  trojan   \n4  ret ldarg.0 call ret call ldsfld callvirt stlo...     181  trojan   \n\n   first_seen  novelty    id  \n0  2018-06-18        0  True  \n1  2018-06-19        0  True  \n2  2018-06-19        0  True  \n3  2018-06-19        1  True  \n4  2018-06-18        0  True  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# 确定异常检测阈值\n",
    "fpr, tpr, thresholds = roc_curve(labels_test_1, discs_1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "for i in range(len(fpr)):\n",
    "    if fpr[i] + tpr[i] >= 1:\n",
    "        i = i - 1\n",
    "        break\n",
    "print(fpr[i])\n",
    "score = thresholds[i]\n",
    "print(\"score: \", score)\n",
    "\n",
    "bool_ = discs_1 >= score\n",
    "# print(type(bool_))\n",
    "# for i in bool_.tolist():\n",
    "#     if i == True:\n",
    "#         print(i)\n",
    "print(len(data_test_1_names), len(bool_.tolist()))\n",
    "data_ood_names = [item for item, value in zip(data_test_1_names, bool_) if value == True] \n",
    "\n",
    "data_ood = data_test_1[data_test_1.name.isin(data_ood_names)]\n",
    "\n",
    "data_ood = data_ood.reset_index(drop=True)\n",
    "print(data_ood.head(5))\n",
    "print(data_test_1.head(5))\n",
    "\n",
    "\n",
    "data_ood.to_csv(\"oodFromTwoHead.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "labels_pre_id = data_test_1['label'].map(lambda x: name2label[x])\n",
    "labels_pre_id = np.array(labels_pre_id).reshape(-1, 1)\n",
    "labels_pre_id = labels_pre_id[discs<thresholds[i]]\n",
    "\n",
    "model_pre = keras.models.load_model('./model/textcnn_split.h5')\n",
    "bool_code = discs<thresholds[i]\n",
    "codes_pre_id = codes_test_1[bool_code.reshape(-1)]\n",
    "y_pred = model_pre.predict(codes_pre_id)\n",
    "y_pred = np.argmax(y_pred, 1)\n",
    "target_names = {'trojan', 'virus', 'worm', 'backdoor'}\n",
    "print(classification_report(labels_pre_id, y_pred, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行参数微调并改进模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 自定义损失函数\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Input, Model, regularizers\n",
    "from tensorflow import keras\n",
    "\n",
    "model = TextCNN(max_len, max_features, embedding_dims).get_model()\n",
    "class DiscrepancyLoss(keras.losses.Loss):\n",
    "    def __init__(self, reduction=keras.losses.Reduction.NONE, \n",
    "                 from_logits=True, name='membership loss'):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "    \n",
    "    def call(self, out1, out2, m=0.3):\n",
    "        out_1 = K.softmax(out1)\n",
    "        out_2 = K.softmax(out2)     \n",
    "        entropy_1 = -out_1 * K.log(out_1)\n",
    "        entropy_2 = -out_2 * K.log(out_2)\n",
    "        entropy_1 = K.mean(entropy_1, axis=1)\n",
    "        entropy_2 = K.mean(entropy_2, axis=1) \n",
    "        \n",
    "        return K.relu(m - K.mean(entropy_1 - entropy_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载SVM预测数据\n",
    "data_test_1 = pd.read_csv('data_test_1.csv')\n",
    "data_test_unsup = data_test_1[data_test_1['novelty']!=0]\n",
    "codes_unsup = data_test_unsup['name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_unsup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codes_generator(codes):\n",
    "    \n",
    "    for idx in range(len(codes)):\n",
    "        fn = codes[idx]\n",
    "        fn = bytes.decode(fn)\n",
    "        op_string = data_csv[data_csv.name == fn]\n",
    "        if len(op_string) == 0:\n",
    "            print('null')\n",
    "            continue\n",
    "        op_string = op_string.iloc[0, 1].split()\n",
    "        res = list(le.transform(op_string))\n",
    "        res = pad_data(res)\n",
    "        yield res\n",
    "\n",
    "def svmdata_generator(codes, labels, mode):\n",
    "    \n",
    "    if mode == 'train':    \n",
    "        codes = codes[: 522]\n",
    "        labels = labels[: 522]\n",
    "    elif mode == 'val':    \n",
    "        codes = codes[522: 1000]\n",
    "        labels = labels[522: 1000]\n",
    "    \n",
    "    labels_res = np.eye(4)[labels]\n",
    "    for idx in range(len(codes)):\n",
    "        fn = codes[idx]\n",
    "        fn = bytes.decode(fn)\n",
    "        op_string = data_csv[data_csv.name == fn]\n",
    "        if len(op_string) == 0:\n",
    "            print('null')\n",
    "            continue\n",
    "        op_string = op_string.iloc[0, 1].split()\n",
    "        res = list(le.transform(op_string))\n",
    "        res = pad_data(res)\n",
    "        yield res, labels_res[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备训练数据\n",
    "batch_size = 64\n",
    "sup_train = tf.data.Dataset.from_generator(svmdata_generator, args=[data_path_1, labels_1, 'train'], \n",
    "                                          output_types=(tf.float32, tf.int32), output_shapes=((None,), (4, )))\n",
    "sup_train = sup_train.batch(batch_size)\n",
    "\n",
    "sup_val = tf.data.Dataset.from_generator(svmdata_generator, args=[data_path_1, labels_1, 'val'], \n",
    "                                          output_types=(tf.float32, tf.int32), output_shapes=((None,), (4, )))\n",
    "sup_val = sup_val.batch(batch_size)\n",
    "\n",
    "unsup_train = tf.data.Dataset.from_generator(codes_generator, args=[codes_unsup], output_types=(tf.float32), output_shapes=((None,)))\n",
    "unsup_train = unsup_train.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备测试数据\n",
    "data_test_1 = pd.read_csv('data_test_1.csv')\n",
    "ood2label = {True: 0, False: 1}\n",
    "labels_test_1 = data_test_1['id'].map(lambda x: ood2label[x])\n",
    "labels_test_1 = labels_test_1.to_list()\n",
    "paths_test_1 = data_test_1['name'].to_list()\n",
    "codes_test_1 = codes_loader(paths_test_1)\n",
    "labels_test_1 = np.array(labels_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "import visdom\n",
    "\n",
    "model = TextCNN(max_len, max_features, embedding_dims).get_model()\n",
    "model.load_weights('./model/textcnn_twohead.h5')\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00005)\n",
    "loss_sup = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "loss_unsup = DiscrepancyLoss()\n",
    "train_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "vis = visdom.Visdom(server='http://localhost')\n",
    "\n",
    "print('>> Fine-tune a Model.')\n",
    "best_auc = 0.\n",
    "num_epochs = 10\n",
    "iters = 0\n",
    "plot_data = {'X': [], 'Y': [], 'legend': ['Sup. Loss', 'Unsup. Loss', 'Tot. Loss']}\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(zip(sup_train, unsup_train)):\n",
    "        \n",
    "        (x_batch_train, y_batch_train), (x_batch_train_unsup) = batch\n",
    "#         print('Start of epoch %d step %d' % (epoch,step))\n",
    "        with tf.GradientTape() as tape:\n",
    "            out_1, out_2 = model(x_batch_train)\n",
    "            loss_sup_value = (loss_sup(y_batch_train, out_1) + loss_sup(y_batch_train, out_2)) / 2\n",
    "            out_1, out_2 = model(x_batch_train_unsup)\n",
    "            loss_unsup_value = loss_unsup(out_1, out_2)\n",
    "            loss_value = loss_sup_value + loss_unsup_value\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # 可视化\n",
    "        if step % 2 == 0:\n",
    "            \n",
    "            loss_sup_value = loss_sup_value.numpy()\n",
    "            loss_unsup_value = loss_unsup_value.numpy()\n",
    "            loss_value = loss_value.numpy()\n",
    "            \n",
    "            plot_data['X'].append(iters)\n",
    "            plot_data['Y'].append([\n",
    "                loss_sup_value, loss_unsup_value, loss_value])\n",
    "            vis.line(\n",
    "                X=np.stack([np.array(plot_data['X'])] * len(plot_data['legend']), 1),\n",
    "                Y=np.array(plot_data['Y']),\n",
    "                opts={\n",
    "                    'title': 'Loss over Time',\n",
    "                    'legend': plot_data['legend'],\n",
    "                    'xlabel': 'Iterations',\n",
    "                    'ylabel': 'Loss',\n",
    "                    'width': 900,\n",
    "                    'height': 300,\n",
    "                },\n",
    "                win=2\n",
    "            )\n",
    "        iters += 1 \n",
    "        \n",
    "        # 进行验证\n",
    "        if step % 2 == 0:\n",
    "            out_1, out_2 = model.predict(codes_test_1)\n",
    "            out_1 = keras.backend.softmax(out_1).numpy()\n",
    "            out_2 = keras.backend.softmax(out_2).numpy()\n",
    "            \n",
    "            entropy_1 = np.max(out_1, axis=1)\n",
    "            entropy_2 = np.max(out_2, axis=1)\n",
    "            \n",
    "#             entropy_1 = keras.backend.mean(entropy_1, axis=1)\n",
    "#             entropy_2 = keras.backend.mean(entropy_2, axis=1)\n",
    "        \n",
    "            labels = np.array(labels_test_1)\n",
    "            discs = np.abs(entropy_1 - entropy_2).reshape((-1, 1))\n",
    "            test_auc = metrics.roc_auc_score(labels, discs)\n",
    "            print('Test AUROC: {:.3f}'.format(test_auc))\n",
    "            if test_auc > best_auc:\n",
    "                best_auc = test_auc\n",
    "                model.save('./model/twohead_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./model/twohead_best.h5')\n",
    "\n",
    "from sklearn import metrics\n",
    "auc_res, discs = get_auc(model, codes_test_1, labels_test_1)\n",
    "print('Final AUROC: {:.3f}'.format(auc_res))\n",
    "plot_entropy(discs, labels_test_1, 'entropy_distribution_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(labels_test_1, discs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "for i in range(len(fpr)):\n",
    "    if fpr[i] + tpr[i] >= 1:\n",
    "        i = i - 1\n",
    "        break\n",
    "print(fpr[i])\n",
    "\n",
    "# 画图，只需要plt.plot(fpr,tpr),变量roc_auc只是记录auc的值，通过auc()函数能计算出来\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC (area = %0.2f) threshold = %f' % ( roc_auc, thresholds[i]))\n",
    "# 画对角线\n",
    "plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('twohead_roc')\n",
    "\n",
    "pred = discs>thresholds[i]\n",
    "pred = pred.astype(int)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels_test_1, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "labels_pre_id = data_test_1['label'].map(lambda x: name2label[x])\n",
    "labels_pre_id = np.array(labels_pre_id).reshape(-1, 1)\n",
    "labels_pre_id = labels_pre_id[discs<thresholds[i]]\n",
    "\n",
    "model_pre = keras.models.load_model('./model/textcnn_split.h5')\n",
    "bool_code = discs<thresholds[i]\n",
    "codes_pre_id = codes_test_1[bool_code.reshape(-1)]\n",
    "y_pred = model_pre.predict(codes_pre_id)\n",
    "y_pred = np.argmax(y_pred, 1)\n",
    "target_names = {'trojan', 'virus', 'worm', 'backdoor'}\n",
    "print(classification_report(labels_pre_id, y_pred, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_test_1, pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对第二个子集进行测试\n",
    "\n",
    "model.load_weights('./model/twohead_best.h5')\n",
    "from sklearn import metrics\n",
    "\n",
    "data_test_2 = pd.read_csv('data_test_2.csv')\n",
    "ood2label = {True: 0, False: 1}\n",
    "labels_test_2 = data_test_2['id'].map(lambda x: ood2label[x])\n",
    "labels_test_2 = labels_test_2.to_list()\n",
    "paths_test_2 = data_test_2['name'].to_list()\n",
    "codes_test_2 = codes_loader(paths_test_2)\n",
    "labels_test_2 = np.array(labels_test_2)\n",
    "auc_res, discs = get_auc(model, codes_test_2, labels_test_2)\n",
    "print('Final AUROC: {:.3f}'.format(auc_res))\n",
    "plot_entropy(discs, labels_test_2, 'entropy_distribution_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(labels_test_2, discs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "for i in range(len(fpr)):\n",
    "    if fpr[i] + tpr[i] >= 1:\n",
    "        i = i - 1\n",
    "        break\n",
    "print(fpr[i])\n",
    "\n",
    "# 画图，只需要plt.plot(fpr,tpr),变量roc_auc只是记录auc的值，通过auc()函数能计算出来\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC (area = %0.2f) threshold = %f' % ( roc_auc, thresholds[i]))\n",
    "# 画对角线\n",
    "plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('twohead_roc_2')\n",
    "\n",
    "pred = discs>thresholds[i]\n",
    "pred = pred.astype(int)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels_test_2, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "labels_pre_id = data_test_2['label'].map(lambda x: name2label[x])\n",
    "labels_pre_id = np.array(labels_pre_id).reshape(-1, 1)\n",
    "labels_pre_id = labels_pre_id[discs<thresholds[i]]\n",
    "\n",
    "model_pre = keras.models.load_model('./model/textcnn_split.h5')\n",
    "bool_code = discs<thresholds[i]\n",
    "codes_pre_id = codes_test_2[bool_code.reshape(-1)]\n",
    "y_pred = model_pre.predict(codes_pre_id)\n",
    "y_pred = np.argmax(y_pred, 1)\n",
    "target_names = {'trojan', 'virus', 'worm', 'backdoor'}\n",
    "print(classification_report(labels_pre_id, y_pred, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_test_2, pred, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd056409600341249f6166a3745098f6a53cb3a4cbc1a38e5a06a7b414fd0ad9715",
   "display_name": "Python 3.8.8 64-bit ('tf2.4': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "56409600341249f6166a3745098f6a53cb3a4cbc1a38e5a06a7b414fd0ad9715"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}