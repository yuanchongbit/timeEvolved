{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers, optimizers, Sequential, metrics\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将2018年的数据作进一步拆分\n",
    "\n",
    "def judge_1(time):\n",
    "    time = time[:7].replace('-', '')\n",
    "    if time <= '201803':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def judge_2(time):\n",
    "    time = time[:7].replace('-', '')\n",
    "    if time > '201803':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "data_csv = pd.read_csv('../../../csv/dataset.csv')\n",
    "data_2017 = data_csv[data_csv['first_seen'].apply(lambda x: x[:4]) == '2017']\n",
    "data_2018 = data_csv[data_csv['first_seen'].apply(lambda x: x[:4]) == '2018']\n",
    "data_2019 = data_csv[data_csv['first_seen'].apply(lambda x: x[:4]) == '2019']\n",
    "data_2018_1 = data_2018[data_2018['first_seen'].apply(judge_1)]\n",
    "data_2018_2 = data_2018[data_2018['first_seen'].apply(judge_2)]\n",
    "\n",
    "data_train = data_2017\n",
    "data_train = data_train.sample(frac=1, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2label = {'trojan':0, 'virus':1, 'worm':2, 'backdoor':3}\n",
    "\n",
    "# 训练数据\n",
    "codes_train = data_train['name'].to_list()\n",
    "labels_train = data_train['label'].map(lambda x: name2label[x])\n",
    "labels_train = labels_train.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(res, max_len=200000):\n",
    "    \n",
    "    length = len(res)\n",
    "    if length > max_len:\n",
    "        return res[ :max_len]\n",
    "    elif length < max_len:\n",
    "        return res + [0]*(max_len-length)\n",
    "    return res\n",
    "\n",
    "\n",
    "def load_data(codes, labels, mode):\n",
    "    \n",
    "    if mode == 'train':    \n",
    "        codes = codes[: 3000]\n",
    "        labels = labels[: 3000]\n",
    "    elif mode == 'val':    \n",
    "        codes = codes[3000: ]\n",
    "        labels = labels[3000: ]\n",
    "    \n",
    "    labels = np.eye(4)[labels]\n",
    "    for idx in range(len(codes)):\n",
    "        fn = codes[idx]\n",
    "        fn = bytes.decode(fn)\n",
    "        fn = '../../../dataset/' + fn[8: ]\n",
    "        if not os.path.isfile(fn):\n",
    "            print(fn, 'not exist')\n",
    "        else:\n",
    "            with open(fn, 'rb') as f:\n",
    "                res = f.read()\n",
    "                res = [byte for byte in res]\n",
    "                res = pad_data(res)\n",
    "                yield res, labels[idx]\n",
    "\n",
    "                \n",
    "def load_test_data(codes, labels):\n",
    "    \n",
    "    for idx in range(len(codes)):\n",
    "        fn = codes[idx]\n",
    "        fn = bytes.decode(fn)\n",
    "        fn = '../../../dataset/' + fn[8: ]\n",
    "        if not os.path.isfile(fn):\n",
    "            print(fn, 'not exist')\n",
    "        else:\n",
    "            with open(fn, 'rb') as f:\n",
    "                res = f.read()\n",
    "                res = [byte for byte in res]\n",
    "                res = pad_data(res)\n",
    "                yield res, labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络结构\n",
    "from tensorflow.keras import Input, Model, regularizers\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "max_len = 200000\n",
    "win_size = 500\n",
    "\n",
    "x = Input((max_len,))\n",
    "emb = layers.Embedding(256, 8)(x)\n",
    "conv1 = layers.Conv1D(kernel_size\n",
    "                      =(win_size), filters=128, strides=(win_size), padding='same')(emb)\n",
    "conv2 = layers.Conv1D(kernel_size=(win_size), filters=128, strides=(win_size), padding='same')(emb)\n",
    "a = layers.Activation('sigmoid', name='sigmoid')(conv2)\n",
    "mul = layers.multiply([conv1, a])\n",
    "a = layers.Activation('relu', name='relu')(mul)\n",
    "p = layers.GlobalMaxPool1D()(a)\n",
    "d = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01))(p)\n",
    "# out1 = layers.Dense(4, activation='softmax')(d)\n",
    "# out2 = layers.Dense(4, activation='softmax')(d)\n",
    "# model = Model(inputs=x, outputs=[out1, out2])\n",
    "out = layers.Dense(4, activation=None)(d)\n",
    "model = Model(inputs=x, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 24 steps, validate for 4 steps\n",
      "Epoch 1/6\n",
      "24/24 [==============================] - 464s 19s/step - loss: 1.7730 - categorical_accuracy: 0.6898 - val_loss: 1.3253 - val_categorical_accuracy: 0.8906\n",
      "Epoch 2/6\n",
      "24/24 [==============================] - 449s 19s/step - loss: 1.0744 - categorical_accuracy: 0.8880 - val_loss: 0.8129 - val_categorical_accuracy: 0.9590\n",
      "Epoch 3/6\n",
      "24/24 [==============================] - 434s 18s/step - loss: 0.7194 - categorical_accuracy: 0.9467 - val_loss: 0.5686 - val_categorical_accuracy: 0.9785\n",
      "Epoch 4/6\n",
      "24/24 [==============================] - 455s 19s/step - loss: 0.5203 - categorical_accuracy: 0.9750 - val_loss: 0.4382 - val_categorical_accuracy: 0.9902\n",
      "Epoch 5/6\n",
      "24/24 [==============================] - 440s 18s/step - loss: 0.3949 - categorical_accuracy: 0.9872 - val_loss: 0.3324 - val_categorical_accuracy: 0.9883\n",
      "Epoch 6/6\n",
      "24/24 [==============================] - 425s 18s/step - loss: 0.3089 - categorical_accuracy: 0.9936 - val_loss: 0.2686 - val_categorical_accuracy: 0.9961\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128  \n",
    "db_train = tf.data.Dataset.from_generator(load_data, args=[codes_train, labels_train, 'train'], \n",
    "                                          output_types=(tf.float32, tf.int32), output_shapes=((None,), (4, )))\n",
    "db_train = db_train.batch(batch_size).repeat(8)\n",
    "\n",
    "db_val = tf.data.Dataset.from_generator(load_data, args=[codes_train, labels_train, 'val'], \n",
    "                                          output_types=(tf.float32, tf.int32), output_shapes=((None,), (4, )))\n",
    "db_val = db_val.batch(batch_size).repeat(6)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "             metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "history = model.fit(db_train,\n",
    "                    epochs=6,\n",
    "                    steps_per_epoch=24,\n",
    "                    validation_data=db_val,\n",
    "                    validation_steps = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./out/malconv_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./out/malconv_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class MembershipLoss(keras.losses.Loss):\n",
    "    \n",
    "    def __init__(self, reduction=keras.losses.Reduction.NONE, \n",
    "                 from_logits=True, name='membership loss'):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "    \n",
    "    def call(self, y_true, y_pred, gamma = 5.0):\n",
    "        y_pred = K.sigmoid(y_pred)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        pt_1 = K.sum(K.pow((1. - pt_1), 2), axis=1)\n",
    "        pt_0 = gamma*(K.mean(K.pow(pt_0, 2), axis=1))\n",
    "        cc = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        cc_value = cc(y_true, y_pred)\n",
    "        \n",
    "        return K.mean(pt_1+pt_0) + cc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据集\n",
    "\n",
    "ood2label = {True: 0, False: 1}\n",
    "data_test_1 = pd.read_csv('data_test_1.csv')\n",
    "codes_test_1 = data_test_1['name'].to_list()\n",
    "labels_test_1 = data_test_1['ID'].map(lambda x: ood2label[x])\n",
    "labels_test_1 = labels_test_1.to_list()\n",
    "\n",
    "batch_size = 128\n",
    "sup_train = tf.data.Dataset.from_generator(load_data, args=[codes_train, labels_train, 'train'], \n",
    "                                          output_types=(tf.float32, tf.int32), output_shapes=((None,), (4, )))\n",
    "sup_train = sup_train.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "sup_val = tf.data.Dataset.from_generator(load_data, args=[codes_train, labels_train, 'val'], \n",
    "                                          output_types=(tf.float32, tf.int32), output_shapes=((None,), (4, )))\n",
    "sup_val = sup_val.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "\n",
    "# unsup_train = tf.data.Dataset.from_generator(load_test_data, args=[codes_test_1, labels_test_1], \n",
    "#                                           output_types=(tf.float32, tf.int32), output_shapes=((None,), ()))\n",
    "# unsup_train = unsup_train.batch(batch_size)\n",
    "\n",
    "# codes_val, labels_val = load_val_data(codes_test_1, labels_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Fine-tune a Model.\n",
      "Start of epoch 0 step 0\n",
      "Start of epoch 0 step 1\n",
      "Start of epoch 0 step 2\n",
      "Start of epoch 0 step 3\n",
      "Start of epoch 0 step 4\n",
      "Start of epoch 0 step 5\n",
      "Start of epoch 0 step 6\n",
      "Start of epoch 0 step 7\n",
      "Start of epoch 0 step 8\n",
      "Start of epoch 0 step 9\n",
      "Start of epoch 0 step 10\n",
      "Start of epoch 0 step 11\n",
      "Start of epoch 0 step 12\n",
      "Start of epoch 0 step 13\n",
      "Start of epoch 0 step 14\n",
      "Start of epoch 0 step 15\n",
      "Start of epoch 0 step 16\n",
      "Start of epoch 0 step 17\n",
      "Start of epoch 0 step 18\n",
      "Start of epoch 0 step 19\n",
      "Start of epoch 0 step 20\n",
      "Start of epoch 0 step 21\n",
      "Start of epoch 0 step 22\n",
      "Start of epoch 0 step 23\n",
      "Start of epoch 0 step 24\n",
      "Start of epoch 0 step 25\n",
      "Start of epoch 0 step 26\n",
      "Validation acc: 0.9997106194496155\n",
      "Start of epoch 1 step 0\n",
      "Start of epoch 1 step 1\n",
      "Start of epoch 1 step 2\n",
      "Start of epoch 1 step 3\n",
      "Start of epoch 1 step 4\n",
      "Start of epoch 1 step 5\n",
      "Start of epoch 1 step 6\n",
      "Start of epoch 1 step 7\n",
      "Start of epoch 1 step 8\n",
      "Start of epoch 1 step 9\n",
      "Start of epoch 1 step 10\n",
      "Start of epoch 1 step 11\n",
      "Start of epoch 1 step 12\n",
      "Start of epoch 1 step 13\n",
      "Start of epoch 1 step 14\n",
      "Start of epoch 1 step 15\n",
      "Start of epoch 1 step 16\n",
      "Start of epoch 1 step 17\n",
      "Start of epoch 1 step 18\n",
      "Start of epoch 1 step 19\n",
      "Start of epoch 1 step 20\n",
      "Start of epoch 1 step 21\n",
      "Start of epoch 1 step 22\n",
      "Start of epoch 1 step 23\n",
      "Start of epoch 1 step 24\n",
      "Start of epoch 1 step 25\n",
      "Start of epoch 1 step 26\n",
      "Validation acc: 0.9997106194496155\n",
      "Start of epoch 2 step 0\n",
      "Start of epoch 2 step 1\n",
      "Start of epoch 2 step 2\n",
      "Start of epoch 2 step 3\n",
      "Start of epoch 2 step 4\n",
      "Start of epoch 2 step 5\n",
      "Start of epoch 2 step 6\n",
      "Start of epoch 2 step 7\n",
      "Start of epoch 2 step 8\n",
      "Start of epoch 2 step 9\n",
      "Start of epoch 2 step 10\n",
      "Start of epoch 2 step 11\n",
      "Start of epoch 2 step 12\n",
      "Start of epoch 2 step 13\n",
      "Start of epoch 2 step 14\n",
      "Start of epoch 2 step 15\n",
      "Start of epoch 2 step 16\n",
      "Start of epoch 2 step 17\n",
      "Start of epoch 2 step 18\n",
      "Start of epoch 2 step 19\n",
      "Start of epoch 2 step 20\n",
      "Start of epoch 2 step 21\n",
      "Start of epoch 2 step 22\n",
      "Start of epoch 2 step 23\n",
      "Start of epoch 2 step 24\n",
      "Start of epoch 2 step 25\n",
      "Start of epoch 2 step 26\n",
      "Validation acc: 1.0\n",
      "Start of epoch 3 step 0\n",
      "Start of epoch 3 step 1\n",
      "Start of epoch 3 step 2\n",
      "Start of epoch 3 step 3\n",
      "Start of epoch 3 step 4\n",
      "Start of epoch 3 step 5\n",
      "Start of epoch 3 step 6\n",
      "Start of epoch 3 step 7\n",
      "Start of epoch 3 step 8\n",
      "Start of epoch 3 step 9\n",
      "Start of epoch 3 step 10\n",
      "Start of epoch 3 step 11\n",
      "Start of epoch 3 step 12\n",
      "Start of epoch 3 step 13\n",
      "Start of epoch 3 step 14\n",
      "Start of epoch 3 step 15\n",
      "Start of epoch 3 step 16\n",
      "Start of epoch 3 step 17\n",
      "Start of epoch 3 step 18\n",
      "Start of epoch 3 step 19\n",
      "Start of epoch 3 step 20\n",
      "Start of epoch 3 step 21\n",
      "Start of epoch 3 step 22\n",
      "Start of epoch 3 step 23\n",
      "Start of epoch 3 step 24\n",
      "Start of epoch 3 step 25\n",
      "Start of epoch 3 step 26\n",
      "Validation acc: 1.0\n",
      "Start of epoch 4 step 0\n",
      "Start of epoch 4 step 1\n",
      "Start of epoch 4 step 2\n",
      "Start of epoch 4 step 3\n",
      "Start of epoch 4 step 4\n",
      "Start of epoch 4 step 5\n",
      "Start of epoch 4 step 6\n",
      "Start of epoch 4 step 7\n",
      "Start of epoch 4 step 8\n",
      "Start of epoch 4 step 9\n",
      "Start of epoch 4 step 10\n",
      "Start of epoch 4 step 11\n",
      "Start of epoch 4 step 12\n",
      "Start of epoch 4 step 13\n",
      "Start of epoch 4 step 14\n",
      "Start of epoch 4 step 15\n",
      "Start of epoch 4 step 16\n",
      "Start of epoch 4 step 17\n",
      "Start of epoch 4 step 18\n",
      "Start of epoch 4 step 19\n",
      "Start of epoch 4 step 20\n",
      "Start of epoch 4 step 21\n",
      "Start of epoch 4 step 22\n",
      "Start of epoch 4 step 23\n",
      "Start of epoch 4 step 24\n",
      "Start of epoch 4 step 25\n",
      "Start of epoch 4 step 26\n",
      "Validation acc: 1.0\n",
      "Start of epoch 5 step 0\n",
      "Start of epoch 5 step 1\n",
      "Start of epoch 5 step 2\n",
      "Start of epoch 5 step 3\n",
      "Start of epoch 5 step 4\n",
      "Start of epoch 5 step 5\n",
      "Start of epoch 5 step 6\n",
      "Start of epoch 5 step 7\n",
      "Start of epoch 5 step 8\n",
      "Start of epoch 5 step 9\n",
      "Start of epoch 5 step 10\n",
      "Start of epoch 5 step 11\n",
      "Start of epoch 5 step 12\n",
      "Start of epoch 5 step 13\n",
      "Start of epoch 5 step 14\n",
      "Start of epoch 5 step 15\n",
      "Start of epoch 5 step 16\n",
      "Start of epoch 5 step 17\n",
      "Start of epoch 5 step 18\n",
      "Start of epoch 5 step 19\n",
      "Start of epoch 5 step 20\n",
      "Start of epoch 5 step 21\n",
      "Start of epoch 5 step 22\n",
      "Start of epoch 5 step 23\n",
      "Start of epoch 5 step 24\n",
      "Start of epoch 5 step 25\n",
      "Start of epoch 5 step 26\n",
      "Validation acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "import visdom\n",
    "from evaluate import evaluate\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_sup = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "loss_ml = MembershipLoss()\n",
    "train_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "vis = visdom.Visdom(server='http://localhost')\n",
    "\n",
    "print('>> Fine-tune a Model.')\n",
    "best_roc = 0.0\n",
    "num_epochs = 6\n",
    "iters = 0\n",
    "plot_data = {'X': [], 'Y': [], 'legend': ['Sup. Loss', 'Ml. Loss', 'Tot. Loss']}\n",
    "for epoch in range(num_epochs):\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(sup_train):\n",
    "        \n",
    "        print('Start of epoch %d step %d' % (epoch,step))\n",
    "        with tf.GradientTape() as tape:\n",
    "            out = model(x_batch_train)\n",
    "            loss_sup_value = loss_sup(y_batch_train, out)\n",
    "            loss_ml_value = loss_ml(y_batch_train, out)\n",
    "            loss_value = loss_sup_value + loss_ml_value\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # 可视化\n",
    "        if step % 5 == 0:\n",
    "            \n",
    "            loss_sup_value = loss_sup_value.numpy()\n",
    "            loss_ml_value = loss_ml_value.numpy()\n",
    "            loss_value = loss_value.numpy()\n",
    "            \n",
    "            plot_data['X'].append(iters)\n",
    "            plot_data['Y'].append([\n",
    "                loss_sup_value, loss_ml_value, loss_value])\n",
    "            vis.line(\n",
    "                X=np.stack([np.array(plot_data['X'])] * len(plot_data['legend']), 1),\n",
    "                Y=np.array(plot_data['Y']),\n",
    "                opts={\n",
    "                    'title': 'Loss over Time',\n",
    "                    'legend': plot_data['legend'],\n",
    "                    'xlabel': 'Iterations',\n",
    "                    'ylabel': 'Loss',\n",
    "                    'width': 1200,\n",
    "                    'height': 390,\n",
    "                },\n",
    "                win=2\n",
    "            )\n",
    "        iters += 1\n",
    "    \n",
    "    for x_batch_val, y_batch_val in sup_val:\n",
    "        val_logits = model(x_batch_val)\n",
    "        val_acc_metric(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print('Validation acc: %s' % (float(val_acc),))\n",
    "#     # 进行验证\n",
    "#     from sklearn import metrics\n",
    "#     out_val = model.predict(codes_val)\n",
    "#     labels = np.array(labels_val)\n",
    "#     dists = np.max(out_val, axis=1).reshape((labels.shape[0], ))\n",
    "#     auc = metrics.roc_auc_score(labels, dists)\n",
    "#     print('Epoch{} AUROC: {:.3f}'.format(epoch, auc))\n",
    "#     if best_auc < auc:\n",
    "#         best_auc = auc\n",
    "#         model.save_weights('./out/malconv_2.h5')\n",
    "#         print('Model saved.')\n",
    "# print('>> Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./out/malconv_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./out/malconv_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3476/3476 [05:04<00:00, 11.40it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1048/1048 [00:35<00:00, 29.28it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_test_data(codes, labels):\n",
    "    \n",
    "    res_all = []\n",
    "    for idx in tqdm(range(len(codes))):\n",
    "        fn = codes[idx]\n",
    "        fn = '../../../dataset/' + fn[8: ]\n",
    "        if not os.path.isfile(fn):\n",
    "            print(fn, 'not exist')\n",
    "        else:\n",
    "            with open(fn, 'rb') as f:\n",
    "                res = f.read()\n",
    "                res = [byte for byte in res]\n",
    "                res = pad_data(res)\n",
    "                res_all.append(res)\n",
    "    return np.array(res_all), labels\n",
    "\n",
    "codes_train, labels_train = load_test_data(codes_train, labels_train)\n",
    "ood2label = {True: 0, False: 1}\n",
    "data_test_1 = pd.read_csv('data_test_1.csv')\n",
    "codes_test_1 = data_test_1['name'].to_list()\n",
    "labels_test_1 = data_test_1['ID'].map(lambda x: ood2label[x])\n",
    "labels_test_1 = labels_test_1.to_list()\n",
    "codes_test_1, labels_test_1 = load_test_data(codes_test_1, labels_test_1)\n",
    "\n",
    "layer_model = Model(inputs=model.input, outputs=model.layers[8].output)\n",
    "feature_test_8 = layer_model.predict(codes_test_1)\n",
    "feature_train_8 = layer_model.predict(codes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.kpca import kPCA\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.utils import Seedy,param_heatmap,param_scatter\n",
    "\n",
    "model = kPCA(q = 68, sigma = 1.0985)\n",
    "model.fit(feature_train_8) #still using model data\n",
    "test_scores = model.decision_function(feature_test_8)\n",
    "# scores.append(test_scores)\n",
    "test_auc = metrics.roc_auc_score(labels_test_1, test_scores)\n",
    "# test_aucs.append(test_auc)\n",
    "print('test auc:', test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_test_id = codes_test_1[test_scores>model.threshold_]\n",
    "model_pre = keras.models.load_model('../model/malconv_1.h5')\n",
    "labels_test_id = data_test_1['label'].map(lambda x: name2label[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test_id = np.array(labels_test_id)[test_scores>model.threshold_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        worm       0.40      1.00      0.57       137\n",
      "       virus       0.94      0.55      0.70       246\n",
      "      trojan       0.72      0.79      0.76       252\n",
      "    backdoor       1.00      0.19      0.33       159\n",
      "\n",
      "    accuracy                           0.63       794\n",
      "   macro avg       0.77      0.64      0.59       794\n",
      "weighted avg       0.79      0.63      0.62       794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_pre.predict(codes_test_id)\n",
    "y_true = labels_test_id\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "# y_true = np.argmax(y_true, axis=1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "target_names = {'trojan', 'virus', 'worm', 'backdoor'}\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
